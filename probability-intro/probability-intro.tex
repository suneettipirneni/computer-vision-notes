\documentclass{article}
\usepackage{amsmath}
\usepackage{mathptmx}
\usepackage{parskip}
\usepackage{hyperref}

\title{Introduction to probability}
\date{2023, April}
\author{Suneet Tipirneni}

\begin{document}
\maketitle

\section{Random Variables}

A random variable is a variable that represents an uncertain result. Some examples of these are \par

\begin{itemize}
	\item Flipping a coin
	\item The temperature outside
	\item Rolling a die
\end{itemize}

A random variable can be either continous or discrete. Continous variables are variables that are based on real numbers. On the other hand, a discrete variable is one with a set amount of variants. Regardless of whether a variable is discrete or continuous, each possible outcome probability must sum to $1$.

\section{Joint Probability}

Joint probability refers to the combined probility of two or more random variables. These are represented like so in equation \ref{eq:joint}:

\begin{equation} \label{eq:joint}
	Pr\left( x,y \right) 
\end{equation}

Where $x$ and $y$ are two different random variables.

\section{Marginalization}

Given a joint distribution, you can recover the constituents of a joint distribution using marginalization. 

If both random variables in the joint distrution are continuous, then you use integration to recover the distribution of a single variable:
\begin{align}\label{eq:marginalize-disc}
	Pr\left( x \right)=\int Pr\left( x,y \right)dy \\
	Pr\left( y \right) = \int Pr\left( x,y \right)dx
\end{align}

With discrete random variables we use summation instead:
\begin{align}\label{eq:marginalize-cont}
	Pr\left( x \right) = \sum_{y} Pr\left( x,y \right) \\
	Pr\left( y \right) = \sum_{x} Pr\left( x,y \right) 
\end{align}
\noindent
To be more specific, when $\sum_{x}$ is used it means to take sum all possible probabilities of $x$ where $x$ is held at a specific value while all other permutations for all variables are used.

For example consider the following joint distribution between the discrete variables $x$ and $y$:

\begin{table}[htpb]
	\centering
	\caption{Joint probabilities between $x$ and $y$}
	\label{tab:joint}
	\begin{tabular}{ccc}
	 	 & $y=0$  & $y=1$ \\
		\hline
		$x=0$ \vline & 0.2 & 0.1 \\
		$x=1$ \vline & 0.1 & 0.6 \\
	\end{tabular}
\end{table}

In this scenario, to recover the marginal distribution for $x$ we need to calculate $\sum_{y}$:
\begin{align*}
	Pr\left( x=0 \right) =  \sum_{y}=Pr\left( x=0, y=0 \right) + Pr\left( x=0,y=1 \right) \\
	= 0.2 + 0.1 \\
	= 0.3
\end{align*}
\begin{align*}
	Pr\left( x=1 \right) = \sum_{y}=Pr\left( x=1,y=1 \right) + Pr\left( x=1,y=0 \right) \\
	= 0.1 + 0.6 \\
	= 0.7
\end{align*}

This yields the probability distribution for $x$:

\begin{table}[htpb]
	\centering
	\caption{Recovered probability of $x$}
	\label{tab:x-recover}
	\begin{tabular}{cc}
		$x=0$  & $x=1$ \\
		\hline
		0.3 & 0.7

	\end{tabular}
\end{table}

The recovered distributions are called the "marginal distributions"

A more complex example of recovering $Pr\left( x,y \right)$ where $z$ is continuous and $w$ is discrete is shown below:
\begin{align*}
	Pr\left( x,y \right) = \sum_{w} \int Pr\left( w,x,y,z \right)dz
\end{align*}

The first thing done is we integrate the joint distribution with respect to $z$. This yields the marginalized distribution $Pr\left( w,x,y \right)$. Next we sum every possible outcome of $w$, this gives us our result of $Pr\left( x,y \right) $

\section{Conditional Probability}

Conditional probability represents the probability of something happening given a specific condition.

It is represented as the following: $Pr\left( x|y=y^{*} \right) $. Here $y=y^{*}$ means we are pinning $y$ to a specific value $y^{*}$. The $|$ is read as "given". So the equation can be read as "probability of $x$ given $y$ equals $y^{*}$

$Pr\left( x|y=y^* \right) $ is calculated from a joint distribution by holding a variable ($y$ in this case) to a specific value. For example, if we reference table \ref{tab:joint}, we can calculate $Pr\left( x|y=1 \right) $. This yields to the following distribution:

\begin{table}[htpb]
	\centering
	\caption{distribution of $x$ given y=0}
	\label{tab:invalid-x-dist}
	\begin{tabular}{cc}
	$x-0$ & $x=1$\\
	\hline
	0.1 & 0.2
	\end{tabular}
\end{table}

Table \ref{tab:invalid-x-dist}, however, is not a valid distrution because the probabilities do not sum to $1$. As a result, the subset of probabilities needs to be normalized to be a valid distribution.

Note: a comma is used instead of $|$ for situations above as it is not the true condional probability, the comma reprents a slice of a distribution that may not be a proper distribution itself.

\begin{align} \label{eq:cond-prob}
	Pr\left( x|y=y^* \right) =\frac{Pr(x,y=y^*)}{\int Pr\left( x,y=y^* \right)dx } = \frac{Pr\left( x,y=y^* \right) }{Pr\left( y=y^* \right) }
\end{align}

Once again using table \ref{tab:joint}, we can use (\ref{eq:cond-prob}) to get the correct distribution of $Pr\left( x|y=1 \right)$.

\begin{align*}
	Pr\left( x,y=1 \right) = \begin{bmatrix} 0.1 & 0.2 \end{bmatrix}   \\
	\sum_{x} Pr(x,y=1)dx = 0.3
\end{align*}
\begin{align*}
	Pr\left( x|y=y^* \right) = \frac{\begin{bmatrix} 0.1 & 0.2 \end{bmatrix} }{0.3} \\
	= \begin{bmatrix} 0.3\overline{3}& 0.6\overline{6} \end{bmatrix}  
\end{align*}

\begin{table}[htpb]
	\centering
	\caption{Correct distribution of $Pr(x|y=1)$}
	\label{tab:correct-cond-x}

	\begin{tabular}{cc}
	$x=0$ & $x=1$ \\
	\hline
	$0.3\overline{3}$ & $0.6\overline{6}$
	\end{tabular}
\end{table}

Table \ref{tab:correct-cond-x} gives the correct probability distribution, we can verify this because all probabilities sum to one as expected.

For a more concise notation for conditional probabilities $y=y^*$ is replaced with just $y$. So instead of $Pr\left( x|y=y^* \right) $, $Pr\left( x|y \right) $ is used instead. Using this equation \ref{eq:cond-prob} can be simplified to \ref{eq:simple-cond}. 

\begin{align} \label{eq:simple-cond}
	Pr\left( x|y \right) =\frac{Pr\left( x,y \right) }{Pr\left( x \right) }
\end{align}

Furthermore \ref{eq:simple-cond} can be rearragned to provide additional equivalences:
\begin{align}\label{eq:cond-x}
	Pr\left( x,y \right) = Pr\left( x|y \right) Pr(y)
\end{align}
\begin{align}\label{eq:cond-y}
	Pr(x,y)=Pr\left( y|x \right) Pr(x)
\end{align}

When there's more than two variables the process is applied repeatedly

\begin{align*}
	Pr\left( w,x,y,z \right) = Pr\left( w,x,y|x \right) Pr\left( x \right) \\
	= Pr\left( w,x|y,z \right) Pr(y|z) Pr(z) \\
	= Pr\left( w|x,y,z) \right) Pr(x|y,z) Pr\left( z \right) 
\end{align*}

\section{Bayes' Rule}

Using equations \ref{eq:cond-x} and \ref{eq:cond-y} we can create a new relation.
\begin{align}\label{eq:cond-related}
	Pr\left( y|x \right) Pr(x)	= Pr\left( x|y \right) Pr(y)
\end{align}

\ref{eq:cond-related} can be rearranged to
\begin{align}\label{eq:bayes}
	Pr\left( y|x \right) = \frac{Pr\left( x|y \right) Pr\left( y \right) }{Pr\left( x \right) }
\end{align}

Each part of \ref{eq:bayes} has a label

\begin{itemize}
	\item $Pr\left( y|x \right)$ is the posterior which reprents already-known information about $y$ given $x$.
	\item $Pr\left( y \right) $ is the prior, it is what is known about $y$ before $x$ is considered.
	\item $Pr\left( x|y \right) $ is the likelihood.
	\item $Pr\left( x \right) $ is the evidence.
\end{itemize}

\section{Independence}

If the following is true
\begin{align*}
	Pr\left( x|y \right) = Pr(x) \\
	Pr\left( y|x \right) = Pr(y) 
\end{align*}

Then both of the conditional probabilities shown above are known as "independent". $x$ has no effect on $y$ or vice-versa.

If we use this equivalence to with equation \ref{eq:cond-x} we can get \ref{eq:cond-indep}
\begin{align} \label{eq:cond-indep}
	Pr\left( x,y \right) = Pr\left( x|y \right) Pr\left( y \right) = Pr\left( x \right) Pr\left( y \right) 
\end{align}

\section{Expectation}

An expectation function is a function that will give you the average value of an outcome pertaining to a variable. 

Formally, an expectation function $f[\cdot]$ is a function that takes all possible values of a variable $x$, $x^*$ and multiplies each of those values against their conditional probability $Pr\left( x=x^* \right) $

They are defined for the continuous and discrete case in \ref{eq:expectation}

\begin{align}\label{eq:expectation}	
	E[f[x]]=\sum_{x} f[x]Pr(x) \\
	\notag E[f[x]]=\int f[x]Pr(x)dx
\end{align}

As an example we can consider a 6-sided die, where our $x$ variable is the outcome of rolling the die. We can calculate the expectation of this by using the expectation function \ref{eq:expectation}.

\begin{align*}
	E[f[x]]=\left( 1*\frac{1}{6} \right) + \left( 2*\frac{1}{6} \right) + \left( 3*\frac{1}{6} \right) + \left( 4*\frac{1}{6} \right) + \left( 5*\frac{1}{6} \right) + \left( 6*\frac{1}{6} \right) \\
	= 3.5
\end{align*}

Therefore, the expected value for rolling a 6-sided is $3.5$. You may notice that in the case of $f[x]$ it's actually the same value as the mean of all possible values of $x$:

\begin{align*}
	\mu_x = \frac{\sum_{1}^{n}x}{n} \\
	\mu_x = \frac{1 + 2 + 3 + 4 + 5 + 5}{6} \\
	\mu_x = 3.5
\end{align*}

As a result we can assume the expectation function $f[x]$ is the same as calculating the mean $\mu_x$

\section{Problems}

\begin{enumerate}
\item Give a real-world example of a joint distribution $Pr(x, y)$ where $x$ is discrete and $y$ is continuous. \\

One real-world example of this could be the probability of inches of rain and the day of the week. Where $x$ is the day of the week and the amount of rain is $y$.
\\
\item What remains if I marginalize a joint distribution $Pr(v, w, x, y, z)$ over five variables with respect to variables $w$ and $y$? What remains if I marginalize the resulting distribution with respect to $v$? \\

Marginalizing $Pr(v, w, x, y, z)$ with respect to $w$ and $y$ yields $Pr(v, x, z)$. Marginalizing the resulting distribution would yield $Pr(x, z)$.
\\
\item Show that the following relation is true:
\begin{equation*}
	Pr\left( w,x,y,z \right) = Pr\left( x,y \right) Pr\left( z|w,x,y \right) Pr\left( w|x,y \right) 
\end{equation*}
\\
We can use the product rule: \ref{eq:cond-x} to show:
\begin{align*}
	Pr\left( w,x,y,z \right) = Pr(z|w,x,y)Pr\left( w,x,y \right) 
\end{align*}

We use the product rule again \ref{eq:cond-y} to expand $Pr(w,x,y)$:
\begin{align*}
	 Pr\left( w,x,y \right) = Pr\left( w|x,y \right) Pr\left( x,y \right) 
\end{align*}

We can now plug this into the first equation to get:
\begin{align*}
	Pr\left( w,x,y,z \right) = Pr\left( z|w,x,y \right) Pr\left( w|x,y \right) Pr(x,y)	
\end{align*}

\item In my pocket there are two coins. Coin 1 is unbiased, so the likelihood $Pr(h = 1|c = 1)$ of getting heads is 0.5 and the likelihood $Pr(h = 0|c = 1)$ of getting tails is also 0.5. Coin 2 is biased, so the likelihood $Pr(h = 1|c = 2)$ of getting heads is 0.8 and the likelihood $Pr(h=0|c=2)$ of getting tails is 0.2. I reach into my pocket and draw one of the coins at random. There is an equal prior probability I might have picked either coin. I flip the coin and observe a head. Use Bayes’ rule to compute the posterior probability that I chose coin 2. \\

Let's first model this problem using \ref{eq:bayes}, we want to know the probability of $c=2$ given that we have gotten heads ($h=1$):

\begin{align*}
	Pr\left( c=2|h=1 \right) = \frac{Pr\left( h=1|c=2 \right) Pr\left( c=2 \right) }{Pr\left( h=1 \right) }
\end{align*}

Now we compute each of the terms. To calculate the evidence we are using marginalization \ref{eq:marginalize-cont} to recover the distribution of $h$ from the joint distribution.
\begin{align*}
	&\text{Prior}=Pr\left( c=2 \right) = 0.5 \\
	&\text{Likelihood}=Pr\left( h=1|c=2 \right) = 0.8 \\
	&\text{Evidence}=Pr\left( h=1 \right) = Pr(h=1|c=1)Pr(c=2) + Pr\left( h=1|c=2 \right) Pr(c=2)\\
	&= 0.5 \times 0.5 + 0.8 \times 0.5\\
	&= 0.65
\end{align*}


We can now subsitute the computed terms into the equation to solve:
\begin{align*}
	&Pr\left( c=2|h=1 \right) = \frac{0.8 \times 0.5}{0.65} \\
	&=0.62
\end{align*}

The probability of coin 2 being chosen when the outcome is heads, is 0.62.

\item If variables $x$ and $y$ are independent and variables $x$ and $z$ are independent, does it follow that variables $y$ and $z$ are independent?

No, if both $y$ and $z$ are independent of $x$, $y$ and $z$ can still be dependent on each other.

\item Use \ref{eq:cond-prob} to show that when $x$ and $y$ are independent, the marginal distribution $Pr(x)$ is the same as the conditional distribution $Pr(x|y=y*)$ for any $y*$. \\

We need to show that:
\begin{align*}
	Pr\left( x \right) = Pr\left( x|y=y* \right) 
\end{align*}

Which is equal to:
\begin{align*}
	&Pr\left( x \right) = \frac{Pr\left( x,y=y* \right) }{Pr\left( y=y* \right) }
\end{align*}

We then use \ref{eq:cond-indep}:
\begin{align*}
	&Pr\left( x \right) = \frac{Pr\left( x \right) Pr(y)}{Pr\left( y \right) } \\
	&Pr\left( x \right) = Pr\left( x \right) 
\end{align*}

\end{enumerate}

\end{document}
